#!/usr/bin/env python3
"""
Test script for LLM Feedback Generator
"""
import sys
import os

# Add the project root to Python path
sys.path.insert(0, os.path.abspath('.'))

def test_llm_feedback_generator():
    """Test LLM Feedback Generator"""
    print("üß™ Testing LLM Feedback Generator")
    print("=" * 50)
    
    try:
        from ml_architecture.services.llm_feedback_generator import LLMFeedbackGenerator
        
        # Initialize generator
        generator = LLMFeedbackGenerator()
        print("‚úÖ LLM Feedback Generator initialized")
        
        # Test data
        cv_analysis = {
            "skills": ["Python", "Django", "React", "SQL", "Git"],
            "experience": ["Full-stack Developer at TechCorp (2022-2024)"],
            "education": ["Bachelor in Computer Science"]
        }
        
        jd_analysis = {
            "extracted_skills": ["Python", "Django", "React", "Docker", "AWS", "CI/CD"]
        }
        
        matching_analysis = {
            "matching_skills": ["Python", "Django", "React"],
            "missing_skills": ["Docker", "AWS", "CI/CD"],
            "skills_match_score": 50.0
        }
        
        quality_analysis = {
            "quality_score": 0.75,
            "strengths": ["C·∫•u tr√∫c CV r√µ r√†ng", "K·ªπ nƒÉng ph√π h·ª£p"],
            "weaknesses": ["Thi·∫øu kinh nghi·ªám cloud"]
        }
        
        overall_score = 65.0
        job_category = "INFORMATION-TECHNOLOGY"
        job_position = "FULLSTACK_DEVELOPER"
        
        # Generate intelligent feedback
        print("\nüîÑ Generating intelligent feedback...")
        feedback_result = generator.generate_intelligent_feedback(
            cv_analysis=cv_analysis,
            jd_analysis=jd_analysis,
            matching_analysis=matching_analysis,
            quality_analysis=quality_analysis,
            overall_score=overall_score,
            job_category=job_category,
            job_position=job_position
        )
        
        print("\nüìä LLM Feedback Result:")
        print("-" * 30)
        
        if feedback_result:
            print(f"‚úÖ Overall Assessment: {feedback_result.get('overall_assessment', 'N/A')}")
            print(f"‚úÖ Strengths: {feedback_result.get('strengths', [])}")
            print(f"‚úÖ Weaknesses: {feedback_result.get('weaknesses', [])}")
            print(f"‚úÖ Specific Suggestions: {feedback_result.get('specific_suggestions', [])}")
            print(f"‚úÖ Priority Actions: {feedback_result.get('priority_actions', [])}")
            print(f"‚úÖ Encouragement: {feedback_result.get('encouragement', 'N/A')}")
        else:
            print("‚ùå No feedback generated")
        
        # Test quick feedback
        print("\nüîÑ Testing quick feedback...")
        quick_feedback = generator.generate_quick_feedback(overall_score, job_category)
        print(f"‚úÖ Quick Feedback: {quick_feedback}")
        
        print("\nüéâ LLM Feedback Generator test completed successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå Error testing LLM Feedback Generator: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_llm_feedback_generator()
    if success:
        print("\n‚úÖ All tests passed!")
    else:
        print("\n‚ùå Some tests failed!")
        sys.exit(1) 